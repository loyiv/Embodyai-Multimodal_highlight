# Embodyai-Multimodal_highlight
# 机器人与多模态研究论文集

> 本文档按四个类别收录近期相关论文：  
> - VLA
> - Benchmark
> - 世界模型（World Model）  
> - 触觉模态 

---

## VLA

1. **[3D-VLA：三维视觉-语言-动作生成世界模型](https://arxiv.org/abs/2403.09631)**  
   - **第一作者**：Zichao Zhang（马萨诸塞大学阿默斯特分校）  
   - **会议/出版**：ICLR 2024  
   - **机构**：UMass Amherst  
   - **项目网站**：所有内容见 [vis-www.cs.umass.edu/3dvla](https://vis-www.cs.umass.edu/3dvla)  
   - **代码仓库**：`https://github.com/UMass-Embodied-AGI/3D-VLA`

2. **[DexGraspVLA：面向通用灵巧抓取的视觉-语言-动作框架](https://arxiv.org/abs/2502.20900)**  
   - **第一作者**：Pengfei Zhao（清华大学）  
   - **会议/出版**：arXiv 2025  
   - **机构**：清华大学  
   - **项目网站**：[dexgraspvla.github.io](https://dexgraspvla.github.io/)  
   - **代码仓库**：`https://github.com/Psi-Robot/DexGraspVLA`

3. **[FuSe：通过语言对齐微调多模态通用机器人策略](https://arxiv.org/abs/2501.04693)**  
   - **第一作者**：Tianhe Chen（加州大学伯克利分校）  
   - **会议/出版**：ICML 2024  
   - **机构**：UC Berkeley  
   - **项目网站**：[fuse-model.github.io](https://fuse-model.github.io/)

4. **[Gemini Robotics：将AI引入物理世界](https://arxiv.org/abs/2503.20020)**  
   - **第一作者**：Gemini Robotics 团队（Google DeepMind）  
   - **会议/出版**：arXiv 2025  
   - **机构**：Google DeepMind  
   - *(目前无公开项目网站或代码链接)*

---

## 基准（Benchmark）

1. **[EmbodiedBench：多模态大语言模型在视觉驱动的嵌入式智能体上的综合基准（v2）](https://arxiv.org/abs/2502.09560)**  
   - **第一作者**：Shaojie Yao（纽约大学）  
   - **会议/出版**：arXiv 2025（v2）  
   - **机构**：NYU  
   - **项目网站**：[embodiedbench.github.io](https://embodiedbench.github.io)  
   - **代码仓库**：`https://github.com/EmbodiedBench/EmbodiedBench`

2. **[VLABench：面向语言条件下机器人长时程推理的动作基准](https://arxiv.org/abs/2412.18194)**  
   - **第一作者**：Shiduo Zhang（复旦大学）  
   - **会议/出版**：arXiv 2024  
   - **机构**：复旦大学  
   - **项目网站**：[vlabench.github.io](https://vlabench.github.io/)  

---

## 世界模型（World Model）

1. **[Navigation World Models（NWM）](https://arxiv.org/abs/2412.03572)**  
   - **第一作者**：Amir Bar（Meta AI）  
   - **会议/出版**：arXiv 2024  
   - **机构**：Meta AI  
   - **项目网站**：[amirbar.net/nwm](https://amirbar.net/nwm/)  
   - **代码仓库**：`https://github.com/facebookresearch/nwm`

2. **[Text2World：用于符号化世界模型生成的大语言模型基准](https://arxiv.org/abs/2502.13092)**  
   - **第一作者**：Jiawei Wang（麻省理工学院）  
   - **会议/出版**：ACL 2024 长文  
   - **机构**：MIT  
   - **项目网站**：[text-to-world.github.io](https://text-to-world.github.io/)  
   - **代码仓库**：`https://github.com/Aaron617/text2world`

3. **[Cosmos-Transfer1：自适应多模态控制的条件化世界生成](https://arxiv.org/abs/2503.14492)**  
   - **第一作者**：Cosmos-Transfer1 团队（NVIDIA）  
   - **会议/出版**：arXiv 2025（v2）  
   - **机构**：NVIDIA  
   - **项目网站 / 代码**：`https://github.com/nvidia-cosmos/cosmos-transfer1`

---

## 触觉模态（Touch Modality）

1. **[A Touch, Vision, and Language Dataset for Multimodal Alignment](https://arxiv.org/abs/2406.07124)**  
   - **第一作者**：Letian Fu（加州大学伯克利分校）  
   - **会议/出版**：ICML 2024  
   - **机构**：UC Berkeley  
   - **项目网站**：[tactile-vlm.github.io](https://tactile-vlm.github.io)  
   - **代码仓库**：`https://github.com/tactile-vlm/TVL`

2. **[Binding Touch to Everything：学习统一多模态触觉表示（UniTouch）](https://arxiv.org/abs/2403.09623)**  
   - **第一作者**：Fengyu Yang（耶鲁大学）  
   - **会议/出版**：CVPR 2024（开放获取）  
   - **机构**：Yale University  
   - **项目网站**：[cfeng16.github.io/UniTouch](https://cfeng16.github.io/UniTouch/)  
   - **代码仓库**：`https://github.com/cfeng16/UniTouch`

3. **[AnyTouch：跨多种视触传感器学习统一的静动态表示](https://arxiv.org/abs/2502.12191)**  
   - **第一作者**：Ruoxuan Feng（中国人民大学）  
   - **会议/出版**：ICLR 2025（已录用）  
   - **机构**：中国人民大学  
   - **项目网站**：[gewu-lab.github.io/AnyTouch](https://gewu-lab.github.io/AnyTouch/)

4. **[AnyTouch: Learning Unified Static-Dynamic Representation Across Multiple Visuo-Tactile Sensors (镜像)](https://arxiv.org/abs/2503.20020)**  
   - **第一作者**：Peng Cheng（清华大学）  
   - **会议/出版**：arXiv 2025  
   - **机构**：清华大学  
   - *(示例占位，如需增补可替换为相应触觉新论文)*

---



